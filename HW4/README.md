1.1 Сравнение на MNIST\
Изменил в файл models, добавив класс FullyConnectedModel для полносвязной сети, и поставил одинаковые гиперпарамтры для всех моделей.\
В utils.py добавил сохранение графиков.\
Обучил и протестированы три модели на датасете MNIST: полносвязная нейросеть, простая сверточная нейросеть и сверточная нейросеть с residual-блоками.\
Полносвязная нейросеть показала точность на обучающем множестве 97.4% и на тестовом множестве 97.8%.\
Время обучения составило 98 секунд, время инференса 2.21 секунды. Количество обучаемых параметров — 2351456.\
![image](https://github.com/user-attachments/assets/4e6fbad6-c620-4b9f-a7ba-23909841079e)

Простая сверточная нейросеть показала точность на обучающем множестве 99.2% и на тестовом множестве 99.2%.\
Время обучения составило 104 секунды, время инференса 2.78 секунды. Количество обучаемых параметров — 421642.\
![image](https://github.com/user-attachments/assets/1de77a43-2b2e-4fdf-a658-8aaf7714d62d)

Сверточная нейросеть с residual-блоками показала точность на обучающем множестве 99.0% и на тестовом множестве 99.1%.\
Время обучения составило 134 секунды, время инференса 2.29 секунды. Количество обучаемых параметров — 153226.\
![image](https://github.com/user-attachments/assets/e9b991ca-e888-4692-8a64-2a388e7df5e5)

Сравнительные графики для полносвязной и сверточных моделей.\
Сравнение полносвязной с простой CNN\
![image](https://github.com/user-attachments/assets/2c7a5366-ced0-4084-8987-66152f45cbe9)

Сравнение полносвязной с CNN с Residual Block\
![image](https://github.com/user-attachments/assets/0576057f-a433-4da5-acc1-3d60954b630a)

![image](https://github.com/user-attachments/assets/6c8917ad-a7e5-41cd-87cb-7f3b8e9ac1fe)

1.2 Сравнение на CIFAR-10\
В utils.py были реализованы функции визуализации: print_confusion_matrix (таблица ошибок), и plot_gradient_flow — отображение распределения градиентов по слоям.\
В trainer.py при установке флага is_grad_flow=1 визуализация градиентов производится на 5-й эпохе.\
FullyConnectedModel показала следующие результаты: точность на обучении — 46%, на тесте — 49%, время обучения — 92.6 секунд, инференса — 2.55 секунд.\
Количество обучаемых параметров — 820874. Градиенты росли по слоям, но модель страдает от переобучения и плохой обобщающей способности.\
![image](https://github.com/user-attachments/assets/8197f40d-1410-4c1c-9c02-b4287902584e)\
![image](https://github.com/user-attachments/assets/b06c6c7d-64b9-4113-9568-17b4b2361ffd)\
![image](https://github.com/user-attachments/assets/ab347e84-0df7-4722-be17-0f2deffff0bd)

CNN: точность на обучающем множестве — 54%, на тестовом — 55%. Время обучения составило 101.1 секунд, инференс занял 2.55 секунд, параметров — 90506.\
Градиенты затухают ближе к последним слоям. Confusion matrix показывает улучшенное качество классификации.\
![image](https://github.com/user-attachments/assets/f77a15fe-43fd-4190-9e31-2b4dcf1b9fa3)\
![image](https://github.com/user-attachments/assets/62d5e5ca-e69c-4a6f-bc5d-f4d2e71774a2)\
![image](https://github.com/user-attachments/assets/71ffc2cf-8ee6-49f3-a2ed-a1ff055e63ec)

CNNWithResidual показала наилучшие результаты: точность на обучении — 77%, на тесте — 76%, время обучения — 143.1 секунд, инференса — 2.53 секунд.\
Параметров — 153802. Gradient flow стабилен и равномерно распределён по слоям блягодаря блокам с остатком, что предотвращает исчезновение градиентов.\
Confusion matrix показывает наиболее чёткое разделение классов.\
![image](https://github.com/user-attachments/assets/2a6c8bf6-af8a-4f46-9374-77339d557de5)\
![image](https://github.com/user-attachments/assets/e05ff508-17bc-42a0-b8ce-a7466d23bd13)\
![image](https://github.com/user-attachments/assets/ef495f0c-e7d2-47fc-a255-86f0f1dd23e3)

![image](https://github.com/user-attachments/assets/2ab6b1e7-b701-44c4-bd3e-aa48625cb4c1)


2.1 Влияние размера ядра свертки\
Чтобы у меня модели с разными размерами ядер имели одинаковое кол-во параметров, я создал новый универсальный класс NEWCNN в файле models, где использовал 
для разных ядер одинаковое колво фильтров, а чтобы компинсировать кол-во параметров, ведь чем больше ядро тем меньше параметров,
я подбирал такой padding,чтобы их колво было одинаковым по формуле - out = (h-kernel+2*pad)/stride +1. Подробные вычисления в комментариях в коде.
Также в файле utils реализовал визуализацию активации первого слоя.
Формула рецептивного поля - R_total = R_prev + (kernel_n - 1) * (strides1 * strides2...)

Для 3x3:
R0=1
R0 = 1+ (3-1)*1 = 3
R1 = 3 + (3-1) = 5
Итоговое рецептивное поле: 5x5
Для 5x5:
R0 = 1
R1 = 1 + (5 - 1) = 5
R2 = 5 + (5 - 1) = 9
тоговое рецептивное поле: 5x5
Для 7x7
R0 = 1
R1 = 1 + (7 - 1) = 7
R2 = 7 + (7 - 1) = 13
Итоговое рецептивное поле: 13x13
Комбинация 1x1 + 3x3
R0 = 1
R1 = 1 + (1 - 1) = 1
R2 = 1 + (3 - 1) = 3
Итоговое рецептивное поле: 3x3
Вывод: увеличение рецептивного поля даёт более глобальный контекст, но может приводить к потере мелких деталей.
MNIST — простой датасет, поэтому малое поле (3x3) оказалось оптимальным.
Ядро 3x3:
Точность на обучающем наборе — 99.62%,
Точность на тестовом наборе — 98.99%,
Время обучения — 118.5 секунд.
Ядро 5x5:
Точность на обучающем наборе — 99.57%,
Точность на тестовом наборе — 98.83%,
Время обучения — 121.5 секунд.
Ядро 7x7:
Точность на обучающем наборе — 99.54%,
Точность на тестовом наборе — 98.76%,
Время обучения — 154.8 секунд.
Комбинация 1x1 + 3x3:
Точность на обучающем наборе — 99.52%,
Точность на тестовом наборе — 98.69%,
Время обучения — 122.2 секунды.

2.2 Влияние глубины CNN
Для 2 слоев уже и с остаточным блоком у нас уже реализованы классы, поэтому реализуем для 4 и 6 в файле models.py.
Добавил в utils визуализацию feature maps.
Неглубокая CNN (2 conv слоя)
Train Accuracy: 99.17%
Test Accuracy: 99.11%
Время обучения: 106.08 сек
Средняя CNN (4 conv слоя)
Train Accuracy: 99.29%
Test Accuracy: 99.24%
Время обучения: 113.83 сек
Глубокая CNN (6 conv слоев)
Train Accuracy: 99.22%
Test Accuracy: 99.20%
Время обучения: 117.99 сек
CNN с Residual связями
Train Accuracy: 99.13%
Test Accuracy: 99.37%
Время обучения: 139.24 сек
Проблемы глубоких моделей в том, что начинается затухание градиентов, и для того чтобы избавится от этого, используется остаточный блок.
Чтобы модель не забывала исходные данные, мы добавляем их.
Неглубокая CNN
Градиенты присутствуют в обоих слоях, но заметно снижаются ближе к выходу.
Средняя CNN (4 слоя)
Видна тенденция к затуханию градиента во внутренних слоях (особенно 2-й и 3-й сверточные слои).
Глубокая CNN (6+ слоев)
Проблема усиливается: в середине сети градиенты стремятся к нулю и веса начальных слоев обучаются хуже.
CNN с Residual связями
Благодаря остаточным связям, градиенты сохраняются и нет затухания. 
Это говорит о высокой эффективности Residual связей в борьбе с затухающими градиентами.
