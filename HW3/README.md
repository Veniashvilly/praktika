1.1 Сравнение моделей разной глубины (15 баллов)\
При проведении эксперимента с различным количеством слоёв, были получены следующие наблюдения:\
1 слой: Простая линейная модель достигла точности на тесте около 0.9197.\
2 слоя: Добавление одного скрытого слоя значительно улучшило результаты: Test Acc ≈ 0.9751, при этом уже наблюдалось небольшое расхождение между train и test.\
3 слоя: Модель достигла Test Acc ≈ 0.9743, а train уже приближался к 0.9840.\
5 слоёв: При увеличении глубины train accuracy достиг 0.9851, а test — 0.9780.\
7 слоёв: Модель обучилась дольше всего (~101 секунда) и достигла Test Acc ≈ 0.9778, при этом train accuracy стал 0.9844.\
Время обучения растёт с глубиной: от ~84 до ~101 секунды.\
Все графики обучения (train/test loss и accuracy по эпохам) сохранены в папку plots.\
Пример графика с 3 слоями из папки plots.\
![image](https://github.com/user-attachments/assets/64b063ec-8ba6-462a-9020-cdd2b29a3c56)


1.2 Анализ переобучения (15 баллов)\
Для исследования влияния глубины модели на переобучение были обучены архитектуры на датасете cifar с теми же конфигурациями что и у nmist и построил графики обучения по метрикам train/test accuracy и loss.\
По результатам логов и графиков, для MNIST оптимальной глубиной оказалась архитектура с 3 слоями (2 скрытых), что подтверждается графиком 3_layers_mnist.png.\
Для CIFAR оптимальной глубиной стала архитектура с 5 слоями (4 скрытых) — см. 5_layers_cifar.png.\
Также были построены аналогичные графики с добавлением Dropout и BatchNorm.\
Включение этих техник улучшило обобщающую способность модели, особенно на CIFAR, где после их добавления тестовая точность значительно увеличилась\
(см. сравнение 5_layers_cifar.png и 5_layers_cifar_with_batch_and_dropout.png). Это говорит о снижении переобучения. Все графики были сохранены в папку plots.\
Пример. 5_layers_cifar.png и 5_layers_cifar_with_batch_and_dropout.png\
![image](https://github.com/user-attachments/assets/b8f53c71-26fc-4d9a-a694-8e59d287006f)\
![image](https://github.com/user-attachments/assets/5a98ed55-806c-46fd-bfa4-e2f7d1c7686c)

2.1 Сравнение моделей разной ширины (15 баллов)\
Модель с узкими слоями ([64, 32, 16]) имела наименьшее количество параметров (≈53 тыс.) и обучалась быстрее всего (~91 сек), но достигла лишь 96.8% точности на тесте.\
Средняя конфигурация ([256, 128, 64], ≈242 тыс. параметров) показала заметный прирост точности — до 97.6%, при умеренном времени обучения (~95 сек).\
Широкая ([1024, 512, 256]) и очень широкая ([2048, 1024, 512]) модели содержали значительно больше параметров (до 4.2 млн), при этом прирост точности был минимален — 97.8% максимум.\
Однако время обучения у них выросло до ~104–107 сек.

2.2 Оптимизация архитектуры (10 баллов)\
![image](https://github.com/user-attachments/assets/438a35de-a015-44b6-a70b-718891242064)

3.1 Сравнение техник регуляризации (15 баллов)\
Проведено сравнение различных техник регуляризации на одинаковой архитектуре. Без регуляризации модель достигла высокой точности (97.6%), но была склонна к переобучению.\
Dropout с коэффициентом 0.1–0.5 стабилизировал обучение, но при сильном значении снижал точность. BatchNorm улучшал устойчивость.\
L2-регуляризация также показала высокую стабильность и точность (до 97.8%), при этом распределение весов стало более компактным.\
![image](https://github.com/user-attachments/assets/b524239f-9032-4a7d-9b3c-9d502a8e1b31)




